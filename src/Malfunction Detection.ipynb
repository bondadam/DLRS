{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malfunction Object Detection Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: opencv-python in c:\\tools\\anaconda3\\lib\\site-packages (4.2.0.34)\nRequirement already satisfied: numpy>=1.14.5 in c:\\users\\heiligeslicht\\appdata\\roaming\\python\\python37\\site-packages (from opencv-python) (1.17.4)\nWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the 'c:\\tools\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OTHER = os.path.join(os.path.abspath(\".\"), \"frames\")\n",
    "PATH = os.path.join(os.path.abspath(\".\"), \"frames\", \"object_detection\")\n",
    "MODEL = os.path.join(os.path.abspath(\".\"), \"malfunction_detection_model\")\n",
    "OBJECTS = os.path.join(MODEL, \"images\")\n",
    "DATA = os.path.join(MODEL, \"data\")\n",
    "CSV_PATH = os.path.join(DATA, \"malfunction_labels.csv\")\n",
    "TRAIN_CSV_PATH = os.path.join(DATA, \"train_labels.csv\")\n",
    "TEST_CSV_PATH = os.path.join(DATA, \"test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(OBJECTS):\n",
    "    rmtree(OBJECTS)\n",
    "os.makedirs(OBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA):\n",
    "    rmtree(DATA)\n",
    "os.makedirs(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_malfunctions(stable_image_path, malfunction_image_path, csv_file):\n",
    "    global COUNTER, OBJECTS\n",
    "    stable = cv2.imread(stable_image_path)\n",
    "    malfunction = cv2.imread(malfunction_image_path)\n",
    "    original = malfunction.copy()\n",
    "    filename = os.path.split(malfunction_image_path)[-1]\n",
    "    # we subtract the malfunction image from the stable one.\n",
    "    image = cv2.absdiff(malfunction, stable)\n",
    "    # we produce a kernel (a 5x5 matrix filled with value 1)\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    # we erode the image to get rid of the contours left behind from the global noise.\n",
    "    image = cv2.erode(image, kernel, iterations=2)\n",
    "    # map the image's colormap to hsv.\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
    "    # we produce a mask by selecting non-black regions of the eroded image.\n",
    "    lower_white = np.array([10,10,10])\n",
    "    upper_white = np.array([255,255,255])\n",
    "    mask = cv2.inRange(image, lower_white, upper_white)\n",
    "    # find the contours of the mask\n",
    "    cnts = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    \n",
    "\n",
    "    def bbArea(c):\n",
    "        _,_,w,h = cv2.boundingRect(c)\n",
    "        return w*h\n",
    "\n",
    "    # Since we have general noise it's best to skip over small captured\n",
    "    # malfunction regions. This is a more suitable solution than trying\n",
    "    # to erode the image for 2 iterations since it makes us lose parts of\n",
    "    # the targeted region. An idea is to use percentiles.\n",
    "    mean = np.array([bbArea(c) for c in cnts]).mean()\n",
    "    offset = 20\n",
    "    wrote_once = False\n",
    "    for c in cnts:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        if w * h > mean:\n",
    "            if not wrote_once:\n",
    "                cv2.imwrite(os.path.join(OBJECTS, filename), original)\n",
    "                wrote_once = True\n",
    "            cv2.rectangle(malfunction, (x - offset, y - offset), (x + w + offset, y + h + offset), (255,0,0), 2)\n",
    "            print(\",\".join(map(str, [filename, original.shape[0], original.shape[1], \"malfunction\", x - offset, y - offset, x + w + offset, y + h + offset])), file=csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import io\n",
    "\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "from collections import namedtuple, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_dir = glob.glob(os.path.join(PATH, \"stable\", \"*.png\"))\n",
    "malfunction_dir = glob.glob(os.path.join(PATH, \"malfunction\",\"*.png\"))\n",
    "\n",
    "stable_prefix = \"stable_\" \n",
    "stable_dir.sort(key=lambda x: int(x[x.index(stable_prefix) + len(stable_prefix): -4]))\n",
    "\n",
    "malfunction_prefix = \"malfunction_\" \n",
    "malfunction_dir.sort(key=lambda x: int(x[x.index(malfunction_prefix) + len(malfunction_prefix): -4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "with open(CSV_PATH, \"w\", encoding=\"utf-8\") as csv_file:\n",
    "    print(\",\".join([\"filename\", \"width\", \"height\", \"class\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"]), file=csv_file)\n",
    "    for (stable_path, malfunction_path) in zip(stable_dir, malfunction_dir):\n",
    "        isolate_malfunctions(stable_path, malfunction_path, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(CSV_PATH)\n",
    "train = data.sample(frac=0.80, random_state=42)\n",
    "train.to_csv(TRAIN_CSV_PATH, sep=',', index=False)\n",
    "test = data.drop(train.index)\n",
    "test.to_csv(TEST_CSV_PATH, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_text_to_int(row_label):\n",
    "    if row_label == 'malfunction':\n",
    "        return 1\n",
    "    else:\n",
    "        None\n",
    "\n",
    "def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "def create_tf_example(group, path):\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'png'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        classes.append(class_text_to_int(row['class']))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfrecords(files):\n",
    "    for file in files:\n",
    "        name = os.path.basename(file).split(\".\")[0]\n",
    "        writer = tf.python_io.TFRecordWriter(os.path.join(DATA, \"{}.record\".format(name)))\n",
    "        path = os.path.join(OBJECTS)\n",
    "        examples = pd.read_csv(file)\n",
    "        grouped = split(examples, 'filename')\n",
    "        for group in grouped:\n",
    "            tf_example = create_tf_example(group, path)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_tfrecords([TRAIN_CSV_PATH, TEST_CSV_PATH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4754469335018131442\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nmemory_limit: 1443813785\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 1864524220320911240\nphysical_device_desc: \"device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n]\n"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_frames_to_video(fileout, path):\n",
    "    frame_array = []\n",
    "    files = glob.glob(os.path.join(*path, \"*.png\"))\n",
    "    prefix = \"malfunction_\"\n",
    "\n",
    "    #for sorting the file names properly\n",
    "    files.sort(key = lambda x: int(x[x.index(prefix)+len(prefix):-4]))\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        filename= files[i]\n",
    "        #reading each files\n",
    "        img = cv2.imread(filename)\n",
    "        height, width, layers = img.shape\n",
    "        size = (width,height)\n",
    "        #inserting the frames into an image array\n",
    "        frame_array.append(img)\n",
    "\n",
    "    out = cv2.VideoWriter(os.path.join(DATA, \"{}.avi\".format(fileout)), cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
    "\n",
    "    for i in range(len(frame_array)):\n",
    "        # writing to a image array\n",
    "        out.write(frame_array[i])\n",
    "    cv2.destroyAllWindows()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "convert_frames_to_video(\"malfunction_frames_train\", [OTHER, \"train\", \"malfunction\"])\n",
    "convert_frames_to_video(\"malfunction_frames_valid\", [OTHER, \"valid\", \"malfunction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}